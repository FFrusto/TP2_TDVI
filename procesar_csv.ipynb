{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier  # Importar el modelo XGBoost\n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np  # Para lidiar con NaNs.\n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "\n",
    "\n",
    "\n",
    "# Score del modelo inicial 0.9464088135807542\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo CSV de entrada \n",
    "archivo_entrada = \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\"\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(archivo_entrada)\n",
    "\n",
    "# Paso la fecha a variable dia, mes, año \n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df['day'] = df['date'].dt.day\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "df.drop(columns=[\"date\"])\n",
    "\n",
    "df.to_csv('competition_data_1.csv', index=False) \n",
    "\n",
    "del df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo CSV de entrada \n",
    "archivo_entrada = \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\"\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(archivo_entrada)\n",
    "\n",
    "# Separo los full_name en categorias y obtengo las dummies \n",
    "df['Categoria'] = df['full_name'].str.split(' -> ').str.get(0)\n",
    "dummies = pd.get_dummies(df['Categoria'])\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "df.to_csv('competition_data_2.csv', index=False) \n",
    "\n",
    "del df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo CSV de entrada \n",
    "archivo_entrada = \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\"\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(archivo_entrada)\n",
    "\n",
    "# Para las categorias binarias, reemplazo los True y False por 1 y 0\n",
    "categorical_columns_to_encode = [\"accepts_mercadopago\", \"boosted\", \"free_shipping\", \"fulfillment\", \"is_pdp\"]\n",
    "for col in categorical_columns_to_encode:\n",
    "    df[col] = df[col].replace({True: 1, False: 0})\n",
    "\n",
    "df.to_csv('competition_data_3.csv', index=False) \n",
    "\n",
    "del df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_3774/2613284890.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['warranty'][fila] = 1\n"
     ]
    }
   ],
   "source": [
    "# Archivo CSV de entrada \n",
    "archivo_entrada = \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\"\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(archivo_entrada)\n",
    "\n",
    "# Me fijo si ofrecen algun tipo de garantía y lo reemplazo por 1 y 0\n",
    "for fila in df['warranty'].index:\n",
    "    if df['warranty'][fila] == 'Sin garant√≠a':\n",
    "        df['warranty'][fila] = 0\n",
    "    elif df['warranty'][fila] == None: \n",
    "        pass\n",
    "    else:\n",
    "        df['warranty'][fila] = 1\n",
    "\n",
    "df.to_csv('competition_data_4.csv', index=False) \n",
    "\n",
    "del df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Archivo CSV de entrada \n",
    "archivo_entrada = \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\"\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(archivo_entrada)\n",
    "\n",
    "#Creo una variable que tiene el porcentaje de descuento que se le hace \n",
    "df['discount'] = (df['original_price'] - df['price']) / df['original_price']\n",
    "\n",
    "df.to_csv('competition_data_5.csv', index=False) \n",
    "\n",
    "del df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_3774/1770094589.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['listing_type_id'][fila] = 1\n",
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_3774/1770094589.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['listing_type_id'][fila] = 2\n"
     ]
    }
   ],
   "source": [
    "# Archivo CSV de entrada \n",
    "archivo_entrada = \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\"\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(archivo_entrada)\n",
    "\n",
    "#Cambio el listing_type_id por 1 y 2\n",
    "\n",
    "for fila in df['listing_type_id'].index:\n",
    "    if df['listing_type_id'][fila] == 'gold_special':\n",
    "        df['listing_type_id'][fila] = 1\n",
    "    elif df['listing_type_id'][fila] == 'gold_pro':\n",
    "        df['listing_type_id'][fila] = 2\n",
    "\n",
    "df.to_csv('competition_data_6.csv', index=False) \n",
    "\n",
    "del df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archivo: /Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\n",
      "Score: 0.9036308985966918\n",
      "Archivo: /Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_1.csv\n",
      "Score: 0.9036308985966918\n",
      "Archivo: /Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_2.csv\n",
      "Score: 0.9027826439727821\n",
      "Archivo: /Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_3.csv\n",
      "Score: 0.9040365855907356\n",
      "Archivo: /Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_4.csv\n",
      "Score: 0.9036308985966918\n",
      "Archivo: /Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_5.csv\n",
      "Score: 0.9025982407936712\n",
      "Archivo: /Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_6.csv\n",
      "Score: 0.9026535617474045\n"
     ]
    }
   ],
   "source": [
    "# Lista de nombres de archivos de datos\n",
    "data_files = [\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\", \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_1.csv\", \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_2.csv\", \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_3.csv\", \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_4.csv\", \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_5.csv\", \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_6.csv\"]\n",
    "\n",
    "for data_file in data_files:\n",
    "    # Cargar el archivo de datos\n",
    "    comp_data = pd.read_csv(data_file)\n",
    "    comp_data[\"date\"] = pd.to_datetime(comp_data[\"date\"])\n",
    "\n",
    "    # Divide la columna de fecha en día, mes y año\n",
    "    comp_data['day'] = comp_data['date'].dt.day\n",
    "    comp_data['month'] = comp_data['date'].dt.month\n",
    "    comp_data['year'] = comp_data['date'].dt.year\n",
    "\n",
    "    # Split into training and evaluation samples\n",
    "    train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "    eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "    del comp_data\n",
    "    gc.collect()\n",
    "\n",
    "    # Dividir los datos en conjunto de entrenamiento y prueba (70% train, 30% test)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include='number'),\n",
    "        train_data[\"conversion\"],\n",
    "        test_size=0.3,  # Proporción para el conjunto de prueba\n",
    "        random_state=42  # Semilla aleatoria para reproducibilidad\n",
    "    )\n",
    "    del train_data\n",
    "    gc.collect()\n",
    "\n",
    "    # Cambiar el modelo a XGBoost\n",
    "    cls = make_pipeline(SimpleImputer(), XGBClassifier(max_depth=10, random_state=2345))\n",
    "    cls.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on the evaluation set\n",
    "    eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "    eval_data = eval_data.select_dtypes(include='number')\n",
    "    y_preds = cls.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, cls.classes_ == 1].squeeze()\n",
    "\n",
    "    print(f\"Archivo: {data_file}\")\n",
    "    print(f\"Score: {cls.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Estos fueron los scores obtenidos en el entrenamiento de un modelo donde se incluía cada uno de los cambios sobre el data frame individualmente. De los resultados obtenidos, podemos ver que todos mejoraron el score del modelo, salvo el 2 en el que se crean variables dummies para las categorias de los productos. \n",
    "Asi, voy a hacer un nuevo data frame que incluya todos los cambios realizados salvo el 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_3774/3693651291.py:25: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['warranty'][fila] = 1\n",
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_3774/3693651291.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['listing_type_id'][fila] = 1\n",
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_3774/3693651291.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['listing_type_id'][fila] = 2\n"
     ]
    }
   ],
   "source": [
    "archivo_entrada = \"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\"\n",
    "\n",
    "# Leer el archivo CSV en un DataFrame\n",
    "df = pd.read_csv(archivo_entrada)\n",
    "\n",
    "# Paso la fecha a variable dia, mes, año \n",
    "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "df['day'] = df['date'].dt.day\n",
    "df['month'] = df['date'].dt.month\n",
    "df['year'] = df['date'].dt.year\n",
    "df.drop(columns=[\"date\"])\n",
    "\n",
    "# Para las categorias binarias, reemplazo los True y False por 1 y 0\n",
    "categorical_columns_to_encode = [\"accepts_mercadopago\", \"boosted\", \"free_shipping\", \"fulfillment\", \"is_pdp\"]\n",
    "for col in categorical_columns_to_encode:\n",
    "    df[col] = df[col].replace({True: 1, False: 0})\n",
    "\n",
    "# Me fijo si ofrecen algun tipo de garantía y lo reemplazo por 1 y 0\n",
    "for fila in df['warranty'].index:\n",
    "    if df['warranty'][fila] == 'Sin garant√≠a':\n",
    "        df['warranty'][fila] = 0\n",
    "    elif df['warranty'][fila] == None: \n",
    "        pass\n",
    "    else:\n",
    "        df['warranty'][fila] = 1\n",
    "\n",
    "#Creo una variable que tiene el porcentaje de descuento que se le hace \n",
    "df['discount'] = (df['original_price'] - df['price']) / df['original_price']\n",
    "\n",
    "#Cambio el listing_type_id por 1 y 2\n",
    "for fila in df['listing_type_id'].index:\n",
    "    if df['listing_type_id'][fila] == 'gold_special':\n",
    "        df['listing_type_id'][fila] = 1\n",
    "    elif df['listing_type_id'][fila] == 'gold_pro':\n",
    "        df['listing_type_id'][fila] = 2\n",
    "\n",
    "\n",
    "\n",
    "df.to_csv('competition_data_7.csv', index=False) \n",
    "\n",
    "del df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entreno un modelo con todo el dataset nuevo \n",
    "# Load the competition data\n",
    "comp_data = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_7.csv\")\n",
    "comp_data[\"date\"] = pd.to_datetime(comp_data[\"date\"])\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include='number'),\n",
    "    train_data[\"conversion\"],\n",
    "    test_size=0.1,  # Proporción para el conjunto de prueba\n",
    "    random_state=42  # Semilla aleatoria para reproducibilidad\n",
    ")\n",
    "del train_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cambiar el modelo a XGBoost\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = 2345, eval_metric = 'auc')\n",
    "clf_xgb.fit(X_train, y_train, eval_set = [(X_test, y_test)], verbose = False)\n",
    "\n",
    "# cls = make_pipeline(SimpleImputer(), XGBClassifier(max_depth=10, random_state=2345))  # Usar XGBClassifier en lugar de DecisionTreeClassifier\n",
    "# cls.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include='number')\n",
    "y_preds =clf_xgb.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, clf_xgb.classes_ == 1].squeeze()\n",
    "\n",
    "# Make the submission file\n",
    "# submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "# submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "# submission_df.to_csv(\"xgboost_model_8.csv\", sep=\",\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9073961387398352\n"
     ]
    }
   ],
   "source": [
    "print(clf_xgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "params = {'max_depth': list(range(1, 40)),\n",
    "          'learning_rate': uniform(scale = 0.2),\n",
    "          'gamma': uniform(scale = 2),\n",
    "          'reg_lambda': uniform(scale = 5),        # Parámetro de regularización.\n",
    "          'subsample': uniform(0.5, 0.5),          # Entre 0.5 y 1.\n",
    "          'min_child_weight': uniform(scale = 5),\n",
    "          'colsample_bytree': uniform(0.75, 0.25), # Entre 0.75 y 1.\n",
    "          'n_estimators': list(range(1, 1000))\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entreno un modelo con todo el dataset nuevo \n",
    "# Load the competition data\n",
    "comp_data = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_7.csv\")\n",
    "comp_data[\"date\"] = pd.to_datetime(comp_data[\"date\"])\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include='number'),\n",
    "    train_data[\"conversion\"],\n",
    "    test_size=0.1,  # Proporción para el conjunto de prueba\n",
    "    random_state=42  # Semilla aleatoria para reproducibilidad\n",
    ")\n",
    "del train_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NO CORRER, Tarda 10 hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor valor de ROC-AUC encontrado: 0.8883394492644392\n",
      "Mejor valor de ROC-AUC encontrado: 0.8941624788503874\n",
      "Mejor valor de ROC-AUC encontrado: 0.8962621064207101\n",
      "Mejor valor de ROC-AUC encontrado: 0.8969607717231346\n",
      "Mejor valor de ROC-AUC encontrado: 0.897251024058803\n",
      "ROC-AUC: 0.89725\n",
      "Grilla: {'colsample_bytree': 0.7579620578446584, 'gamma': 0.7297041792253138, 'learning_rate': 0.021892166283765713, 'max_depth': 13, 'min_child_weight': 1.1724744683543746, 'n_estimators': 784, 'reg_lambda': 1.8656819555055764, 'subsample': 0.8630230443880356}\n",
      "Tiempo transcurrido: 30753.64494395256 segundos\n",
      "Tiempo de entrenamiento por iteración: 307.54 segundos\n"
     ]
    }
   ],
   "source": [
    "random_state = 2345\n",
    "start = time.time()\n",
    "best_score = 0\n",
    "best_estimator = None\n",
    "iterations = 100\n",
    "for g in ParameterSampler(params, n_iter = iterations, random_state = random_state):\n",
    "    clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = random_state, eval_metric = 'auc', **g)\n",
    "    clf_xgb.fit(X_train, y_train, eval_set = [(X_test, y_test)], verbose = False)\n",
    "\n",
    "    y_pred = clf_xgb.predict_proba(X_test)[:, 1] # Obtenemos la probabilidad de una de las clases (cualquiera).\n",
    "    auc_roc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    # Guardamos si es mejor.\n",
    "    if auc_roc > best_score:\n",
    "        print(f'Mejor valor de ROC-AUC encontrado: {auc_roc}')\n",
    "        best_score = auc_roc\n",
    "        best_grid = g\n",
    "        best_estimator = clf_xgb\n",
    "\n",
    "end = time.time()\n",
    "print('ROC-AUC: %0.5f' % best_score)\n",
    "print('Grilla:', best_grid)\n",
    "print(f'Tiempo transcurrido: {str(end - start)} segundos')\n",
    "print(f'Tiempo de entrenamiento por iteración: {str(round((end - start) / iterations, 2))} segundos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entreno un modelo con todo el dataset nuevo Modelo 9 \n",
    "# Load the competition data\n",
    "comp_data = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_7.csv\")\n",
    "\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include=['number','float']),\n",
    "    train_data[\"conversion\"],\n",
    "    test_size=0.1,  # Proporción para el conjunto de prueba\n",
    "    random_state=42  # Semilla aleatoria para reproducibilidad\n",
    ")\n",
    "del train_data\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "params = {'max_depth': 13,\n",
    "          'learning_rate': 0.021892166283765713,\n",
    "          'gamma': 0.7297041792253138,\n",
    "          'reg_lambda': 1.8656819555055764,        # Parámetro de regularización.\n",
    "          'subsample': 0.8630230443880356,          # Entre 0.5 y 1.\n",
    "          'min_child_weight': 1.1724744683543746,\n",
    "          'colsample_bytree': 0.7579620578446584, # Entre 0.75 y 1.\n",
    "          'n_estimators': 784\n",
    "         }\n",
    "\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = 2345, eval_metric = 'auc', **params)\n",
    "clf_xgb.fit(X_train, y_train, eval_set = [(X_test, y_test)], verbose = False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include=['number','float'])\n",
    "y_preds =clf_xgb.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, clf_xgb.classes_ == 1].squeeze()\n",
    "\n",
    "# Make the submission file\n",
    "# submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "# submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "# submission_df.to_csv(\"xgboost_model_9.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9090003872323947\n"
     ]
    }
   ],
   "source": [
    "print(clf_xgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sigo viendo cambios para hacer en el df \n",
    "\n",
    "df = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_7.csv\")\n",
    "dummies = pd.get_dummies(df['platform'])\n",
    "df = pd.concat([df, dummies], axis=1)\n",
    "\n",
    "df.to_csv('competition_data_8.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Entreno un nuevo modelo con este nuevo df\n",
    "\n",
    "# Load the competition data\n",
    "comp_data = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_8.csv\")\n",
    "\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include=['number','float']),\n",
    "    train_data[\"conversion\"],\n",
    "    test_size=0.1,  # Proporción para el conjunto de prueba\n",
    "    random_state=42  # Semilla aleatoria para reproducibilidad\n",
    ")\n",
    "del train_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 10 \n",
    "\n",
    "params = {'max_depth': 13,\n",
    "          'learning_rate': 0.021892166283765713,\n",
    "          'gamma': 0.7297041792253138,\n",
    "          'reg_lambda': 1.8656819555055764,        # Parámetro de regularización.\n",
    "          'subsample': 0.8630230443880356,          # Entre 0.5 y 1.\n",
    "          'min_child_weight': 1.1724744683543746,\n",
    "          'colsample_bytree': 0.7579620578446584, # Entre 0.75 y 1.\n",
    "          'n_estimators': 784\n",
    "         }\n",
    "\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = 2345, eval_metric = 'auc', **params)\n",
    "clf_xgb.fit(X_train, y_train, eval_set = [(X_test, y_test)], verbose = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9103280411572716\n"
     ]
    }
   ],
   "source": [
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include=['number','float'])\n",
    "y_preds =clf_xgb.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, clf_xgb.classes_ == 1].squeeze()\n",
    "\n",
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"xgboost_model_10.csv\", sep=\",\", index=False)\n",
    "\n",
    "print(clf_xgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/franciscofrustoalvarado/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/franciscofrustoalvarado/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funciones para Word2vec\n",
    "\n",
    "STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "def iterate_LN_corpus(path):\n",
    "    \"\"\"\n",
    "    Genera un iterador para recorrer los archivos de texto en un directorio.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta al directorio que contiene los archivos.\n",
    "\n",
    "    Yields:\n",
    "        str: Texto contenido en cada archivo.\n",
    "    \"\"\"\n",
    "    articles = os.listdir(path)\n",
    "    random.shuffle(articles)\n",
    "    for art in articles:\n",
    "        with open(path + art, encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        yield(raw_text)\n",
    "\n",
    "def tokenizer(raw_text):\n",
    "    \"\"\"\n",
    "    Tokeniza y preprocesa un texto.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): Texto sin procesar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de oraciones, donde cada oración es una lista de palabras.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    sentences = [word_tokenize(e) for e in sentences]\n",
    "    sentences = [[e2 for e2 in e1 if re.compile(\"[A-Za-z]\").search(e2[0])] for e1 in sentences]\n",
    "    sentences = [[e2.lower() for e2 in e1] for e1 in sentences]\n",
    "    return(sentences)\n",
    "\n",
    "def gen_sentences(path):\n",
    "    \"\"\"\n",
    "    Genera una lista de oraciones a partir de archivos de texto en un directorio.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta al directorio que contiene los archivos de texto.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de oraciones.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    n_arts = len(os.listdir(path))\n",
    "    for i, art in tqdm.tqdm(enumerate(iterate_LN_corpus(path)), total=n_arts):\n",
    "        sentences.extend(tokenizer(art))\n",
    "    return(sentences)\n",
    "\n",
    "def average_vectors(title_tokens, model, stopwords=None):\n",
    "    \"\"\"\n",
    "    Calcula el vector promedio de un conjunto de tokens utilizando un modelo Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        title_tokens (list): Lista de tokens.\n",
    "        model (gensim.models.Word2Vec): Modelo Word2Vec.\n",
    "        stopwords (set, optional): Conjunto de palabras stopwords. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Vector promedio.\n",
    "    \"\"\"\n",
    "    title_tokens = [e2 for e1 in title_tokens for e2 in e1]\n",
    "    title_tokens = [e for e in title_tokens if e in model.wv]\n",
    "    if stopwords is not None:\n",
    "        title_tokens = [e for e in title_tokens if e not in stopwords]\n",
    "    if len(title_tokens) == 0:\n",
    "        output = np.zeros(model.wv.vector_size)\n",
    "    else:\n",
    "        output = np.array([model.wv.get_vector(e) for e in title_tokens]).mean(0)\n",
    "    return output\n",
    "\n",
    "def dummy_tokenizer(text_tokens):\n",
    "    \"\"\"\n",
    "    Tokenizador dummy que simplemente devuelve los tokens de texto sin procesar.\n",
    "\n",
    "    Args:\n",
    "        text_tokens (list): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: Misma lista de tokens de entrada.\n",
    "    \"\"\"\n",
    "    return text_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Word2vec\n",
    "# Carga de datos desde un archivo CSV\n",
    "\n",
    "comp_data = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_8.csv\")\n",
    "comp_data[\"title_tokens\"] = comp_data[\"title\"].map(tokenizer)\n",
    "\n",
    "# Creación del modelo Word2Vec\n",
    "w2v_tp = gensim.models.Word2Vec(vector_size=300,\n",
    "                                window=3,\n",
    "                                min_count=5,\n",
    "                                negative=15,\n",
    "                                sample=0.01,\n",
    "                                workers=8,\n",
    "                                sg=1)\n",
    "\n",
    "# Creación del vocabulario a partir del corpus\n",
    "w2v_tp.build_vocab([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "                   progress_per=10000)\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "w2v_tp.train([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "             total_examples=w2v_tp.corpus_count,\n",
    "             epochs=30, report_delay=1)\n",
    "\n",
    "# Obtención de embeddings de títulos utilizando el modelo Word2Vec\n",
    "title_embs = comp_data[\"title_tokens\"].map(lambda x: average_vectors(x, w2v_tp, STOP_WORDS_SP))\n",
    "title_embs = np.array(title_embs.to_list())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Armo el csv con toda la info anterior y la codificacion de los titulos \n",
    "titles = pd.DataFrame(title_embs)\n",
    "csv_9 = pd.concat([comp_data, titles], axis=1)\n",
    "\n",
    "\n",
    "csv_9.to_csv('competition_data_9.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Hago un modelo con esta nueva info \n",
    "\n",
    "#Entreno un nuevo modelo con este nuevo df\n",
    "\n",
    "# Load the competition data\n",
    "comp_data = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data_9.csv\")\n",
    "\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "del comp_data\n",
    "gc.collect()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include=['number','float']),\n",
    "    train_data[\"conversion\"],\n",
    "    test_size=0.1,  # Proporción para el conjunto de prueba\n",
    "    random_state=42  # Semilla aleatoria para reproducibilidad\n",
    ")\n",
    "del train_data\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarda 1hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Modelo 11\n",
    "\n",
    "params = {'max_depth': 13,\n",
    "          'learning_rate': 0.021892166283765713,\n",
    "          'gamma': 0.7297041792253138,\n",
    "          'reg_lambda': 1.8656819555055764,        # Parámetro de regularización.\n",
    "          'subsample': 0.8630230443880356,          # Entre 0.5 y 1.\n",
    "          'min_child_weight': 1.1724744683543746,\n",
    "          'colsample_bytree': 0.7579620578446584, # Entre 0.75 y 1.\n",
    "          'n_estimators': 784\n",
    "         }\n",
    "\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = 2345, eval_metric = 'auc', **params)\n",
    "clf_xgb.fit(X_train, y_train, eval_set = [(X_test, y_test)], verbose = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9082259224428832\n"
     ]
    }
   ],
   "source": [
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include=['number','float'])\n",
    "y_preds =clf_xgb.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, clf_xgb.classes_ == 1].squeeze()\n",
    "\n",
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"xgboost_model_11.csv\", sep=\",\", index=False)\n",
    "\n",
    "print(clf_xgb.score(X_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
