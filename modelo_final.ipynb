{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importamos todas las librerias que vamos a usar para el desarollo del trabajo\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBClassifier \n",
    "import xgboost as xgb\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np \n",
    "import time\n",
    "import sklearn\n",
    "from sklearn.metrics import balanced_accuracy_score, roc_auc_score, make_scorer\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import logging\n",
    "import gensim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import uniform\n",
    "import pandas as pd\n",
    "import gc\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import make_pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creación del primer subbmit y primer análisis del dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\")\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = data[data[\"ROW_ID\"].isna()]\n",
    "eval_data = data[data[\"ROW_ID\"].notna()]\n",
    "del data\n",
    "gc.collect()\n",
    "\n",
    "# Train a tree on the train data\n",
    "train_data = train_data.sample(frac=1/3)\n",
    "y_train = train_data[\"conversion\"]\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"])\n",
    "X_train = X_train.select_dtypes(include='number')\n",
    "del train_data\n",
    "gc.collect()\n",
    "\n",
    "cls = make_pipeline(SimpleImputer(), DecisionTreeClassifier(max_depth=8, random_state=2345))\n",
    "cls.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the evaluation set\n",
    "y_true = eval_data[\"conversion\"]\n",
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include='number')\n",
    "y_preds = cls.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, cls.classes_ == 1].squeeze()\n",
    "\n",
    "\n",
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"basic_model.csv\", sep=\",\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "print_position: 0.3941\n",
      "user_id: 0.10544\n",
      "total_orders_item_30days: 0.07388\n",
      "offset: 0.04267\n",
      "available_quantity: 0.03991\n",
      "total_visits_item: 0.03482\n",
      "product_id: 0.02939\n",
      "avg_gmv_item_sel: 0.02708\n",
      "avg_qty_orders_item_domain_30days: 0.0241\n",
      "sold_quantity: 0.02271\n",
      "avg_gmv_seller_bday: 0.02203\n",
      "avg_gmv_item_domain_30days: 0.02044\n",
      "total_visits_seller: 0.01558\n",
      "avg_si_item_sel_30day: 0.01465\n",
      "total_items_seller: 0.01452\n",
      "avg_qty_orders_item_sel_30days: 0.01372\n",
      "total_si_sel_30days: 0.01144\n",
      "price: 0.01092\n",
      "total_asp_item_domain_30days: 0.01049\n",
      "total_si_item_30days: 0.01011\n"
     ]
    }
   ],
   "source": [
    "# Asumiendo que 'importances' contiene la importancia de cada atributo\n",
    "importances = cls.named_steps['decisiontreeclassifier'].feature_importances_\n",
    "\n",
    "# Obtener los índices de los atributos en orden descendente de importancia\n",
    "sorted_indices = np.argsort(importances)[::-1]\n",
    "\n",
    "# Obtener los nombres de los atributos correspondientes a los 20 primeros índices\n",
    "top_20_attributes = X_train.columns[sorted_indices[:20]]\n",
    "\n",
    "# Obtener las importancias correspondientes a los 20 atributos\n",
    "top_20_importances = importances[sorted_indices[:20]]\n",
    "\n",
    "# Imprimir los nombres y las importancias de los 20 atributos más importantes\n",
    "for attribute, importance in zip(top_20_attributes, top_20_importances):\n",
    "    print(f'{attribute}: {importance.round(5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo xgboost_model_20.csv\n",
    "\n",
    "El desarrollo de este modelo, fue un trabajo iterativo en el que para lograr el modelo final, previamente se entrenaron muchos y distintos modelos donde se fue mejorando cada uno y este modelo final es el resultado de la suma de todas las mejoras. \n",
    "Este modelo, entrena en base a 111 atributos que incluyen, nuevas variables creadas, Word2vec de los titulos de cada publicacion y one hot encoding de algunas variables categoricas. Una pequeña obervación, es que al intentar probar hacer OHE con los metodos más eficientes y fáciles, tuvimos muchos errores para poder correrlo, de manera que terminamos haciendolo de una manera más \"manual\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_39328/4137974036.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comp_data['warranty'][fila] = 1\n",
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_39328/4137974036.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comp_data['listing_type_id'][fila] = 1\n",
      "/var/folders/5d/4g206gss6cg3fktfbycqnb540000gn/T/ipykernel_39328/4137974036.py:26: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  comp_data['listing_type_id'][fila] = 2\n"
     ]
    }
   ],
   "source": [
    "# Abrimos el dataset y realizamos las modificaciones\n",
    "\n",
    "comp_data = pd.read_csv(\"/Users/franciscofrustoalvarado/Desktop/TD_VI/TP2_TDVI/competition_data.csv\")\n",
    "\n",
    "# Me fijo si ofrecen algun tipo de garantía y lo reemplazo por 1 y 0\n",
    "for fila in comp_data['warranty'].index:\n",
    "    if comp_data['warranty'][fila] == 'Sin garant√≠a':\n",
    "        comp_data['warranty'][fila] = 0\n",
    "    elif comp_data['warranty'][fila] == None: \n",
    "        pass\n",
    "    else:\n",
    "        comp_data['warranty'][fila] = 1\n",
    "\n",
    "# Paso la fecha a variable dia, mes, año \n",
    "comp_data[\"date\"] = pd.to_datetime(comp_data[\"date\"])\n",
    "comp_data['day'] = comp_data['date'].dt.day\n",
    "comp_data['month'] = comp_data['date'].dt.month\n",
    "comp_data['year'] = comp_data['date'].dt.year\n",
    "comp_data.drop(columns=[\"date\"])\n",
    "\n",
    "#Cambio el listing_type_id por 1 y 2\n",
    "for fila in comp_data['listing_type_id'].index:\n",
    "    if comp_data['listing_type_id'][fila] == 'gold_special':\n",
    "        comp_data['listing_type_id'][fila] = 1\n",
    "    elif comp_data['listing_type_id'][fila] == 'gold_pro':\n",
    "        comp_data['listing_type_id'][fila] = 2\n",
    "\n",
    "#Creo una variable que cuenta la cantidad de tags que tiene cada producto\n",
    "comp_data['tags_count'] = comp_data['tags'].str.count(',')+1\n",
    "\n",
    "#One-hot-enconding\n",
    "categorical_columns_to_encode = [\"accepts_mercadopago\", \"boosted\", \"free_shipping\", \"fulfillment\", \"is_pdp\", \"warranty\"]\n",
    "\n",
    "for col in categorical_columns_to_encode:\n",
    "    # Crear columnas para \"True\", \"False\" y \"NA\"\n",
    "    comp_data[col + '_true'] = (comp_data[col] == True).astype(int)\n",
    "    comp_data[col + '_false'] = (comp_data[col] == False).astype(int)\n",
    "    comp_data[col + '_NA'] = comp_data[col].isna().astype(int)\n",
    "\n",
    "for col in categorical_columns_to_encode:    \n",
    "    comp_data.drop(columns=col, inplace=True)\n",
    "\n",
    "#Creo las dummies según la plataforma a traves de la que se hizo la compra\n",
    "dummies = pd.get_dummies(comp_data['platform'])\n",
    "comp_data = pd.concat([comp_data, dummies], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creacion de nuevas variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creo una variable que tiene el porcentaje de descuento que se le hace \n",
    "comp_data['discount'] = (comp_data['original_price'] - comp_data['price']) / comp_data['original_price']\n",
    "\n",
    "#Creo una variable que tiene el precio al cuadrado\n",
    "comp_data[\"price2\"] = comp_data['price']*comp_data['price']\n",
    "\n",
    "#Creo una variable que relaciona a la catnidad de disponible con la cantidad vendida\n",
    "comp_data['available_sold'] = comp_data['available_quantity']*comp_data['sold_quantity']\n",
    "\n",
    "#Creo una variablee que tiene el promedio de las ventas sobre las visitas que tuvo el item \n",
    "comp_data['promedio_vtas'] = comp_data['sold_quantity']/comp_data['total_visits_item']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hacemos Word2Vec con las funciones provistas por la cátedra \n",
    "\n",
    "STOP_WORDS_SP = set(stopwords.words('spanish'))\n",
    "\n",
    "def iterate_LN_corpus(path):\n",
    "    \"\"\"\n",
    "    Genera un iterador para recorrer los archivos de texto en un directorio.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta al directorio que contiene los archivos.\n",
    "\n",
    "    Yields:\n",
    "        str: Texto contenido en cada archivo.\n",
    "    \"\"\"\n",
    "    articles = os.listdir(path)\n",
    "    random.shuffle(articles)\n",
    "    for art in articles:\n",
    "        with open(path + art, encoding=\"utf-8\") as f:\n",
    "            raw_text = f.read()\n",
    "        yield(raw_text)\n",
    "\n",
    "def tokenizer(raw_text):\n",
    "    \"\"\"\n",
    "    Tokeniza y preprocesa un texto.\n",
    "\n",
    "    Args:\n",
    "        raw_text (str): Texto sin procesar.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de oraciones, donde cada oración es una lista de palabras.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(raw_text)\n",
    "    sentences = [word_tokenize(e) for e in sentences]\n",
    "    sentences = [[e2 for e2 in e1 if re.compile(\"[A-Za-z]\").search(e2[0])] for e1 in sentences]\n",
    "    sentences = [[e2.lower() for e2 in e1] for e1 in sentences]\n",
    "    return(sentences)\n",
    "\n",
    "def gen_sentences(path):\n",
    "    \"\"\"\n",
    "    Genera una lista de oraciones a partir de archivos de texto en un directorio.\n",
    "\n",
    "    Args:\n",
    "        path (str): Ruta al directorio que contiene los archivos de texto.\n",
    "\n",
    "    Returns:\n",
    "        list: Lista de oraciones.\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    n_arts = len(os.listdir(path))\n",
    "    for i, art in tqdm.tqdm(enumerate(iterate_LN_corpus(path)), total=n_arts):\n",
    "        sentences.extend(tokenizer(art))\n",
    "    return(sentences)\n",
    "\n",
    "def average_vectors(title_tokens, model, stopwords=None):\n",
    "    \"\"\"\n",
    "    Calcula el vector promedio de un conjunto de tokens utilizando un modelo Word2Vec.\n",
    "\n",
    "    Args:\n",
    "        title_tokens (list): Lista de tokens.\n",
    "        model (gensim.models.Word2Vec): Modelo Word2Vec.\n",
    "        stopwords (set, optional): Conjunto de palabras stopwords. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        numpy.ndarray: Vector promedio.\n",
    "    \"\"\"\n",
    "    title_tokens = [e2 for e1 in title_tokens for e2 in e1]\n",
    "    title_tokens = [e for e in title_tokens if e in model.wv]\n",
    "    if stopwords is not None:\n",
    "        title_tokens = [e for e in title_tokens if e not in stopwords]\n",
    "    if len(title_tokens) == 0:\n",
    "        output = np.zeros(model.wv.vector_size)\n",
    "    else:\n",
    "        output = np.array([model.wv.get_vector(e) for e in title_tokens]).mean(0)\n",
    "    return output\n",
    "\n",
    "def dummy_tokenizer(text_tokens):\n",
    "    \"\"\"\n",
    "    Tokenizador dummy que simplemente devuelve los tokens de texto sin procesar.\n",
    "\n",
    "    Args:\n",
    "        text_tokens (list): Lista de tokens.\n",
    "\n",
    "    Returns:\n",
    "        list: Misma lista de tokens de entrada.\n",
    "    \"\"\"\n",
    "    return text_tokens\n",
    "\n",
    "\n",
    "comp_data[\"title_tokens\"] = comp_data[\"title\"].map(tokenizer)\n",
    "\n",
    "# Creación del modelo Word2Vec\n",
    "w2v_tp = gensim.models.Word2Vec(vector_size=50,\n",
    "                                window=5,\n",
    "                                min_count=5,\n",
    "                                negative=15,\n",
    "                                sample=0.01,\n",
    "                                workers=8,\n",
    "                                sg=1)\n",
    "\n",
    "# Creación del vocabulario a partir del corpus\n",
    "w2v_tp.build_vocab([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "                   progress_per=10000)\n",
    "\n",
    "# Entrenamiento del modelo Word2Vec\n",
    "w2v_tp.train([e2 for e1 in comp_data[\"title_tokens\"].values for e2 in e1],\n",
    "             total_examples=w2v_tp.corpus_count,\n",
    "             epochs=30, report_delay=1)\n",
    "\n",
    "# Obtención de embeddings de títulos utilizando el modelo Word2Vec\n",
    "title_embs = comp_data[\"title_tokens\"].map(lambda x: average_vectors(x, w2v_tp, STOP_WORDS_SP))\n",
    "title_embs = np.array(title_embs.to_list())\n",
    "\n",
    "comp_data = pd.concat([comp_data, pd.DataFrame(title_embs)], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([               'available_quantity',\n",
      "              'avg_gmv_item_domain_30days',\n",
      "                        'avg_gmv_item_sel',\n",
      "                     'avg_gmv_seller_bday',\n",
      "       'avg_qty_orders_item_domain_30days',\n",
      "          'avg_qty_orders_item_sel_30days',\n",
      "                   'avg_si_item_sel_30day',\n",
      "                                 'benefit',\n",
      "                              'conversion',\n",
      "                                  'health',\n",
      "       ...\n",
      "                                        40,\n",
      "                                        41,\n",
      "                                        42,\n",
      "                                        43,\n",
      "                                        44,\n",
      "                                        45,\n",
      "                                        46,\n",
      "                                        47,\n",
      "                                        48,\n",
      "                                        49],\n",
      "      dtype='object', length=115)\n",
      "115\n"
     ]
    }
   ],
   "source": [
    "print(comp_data.select_dtypes(include=['number','float']).columns)\n",
    "print(len(comp_data.select_dtypes(include=['number','float']).columns.tolist()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Busqueda de hiperparametros (DEMORA 10HS)\n",
    "\n",
    "Este código fue corrido en otro notebook de manera que el resultado printeado está hardcodeada. Además, se el ajuste de los hiperparametros lo hicimos en uno de los primeros pasos del modelo, donde NO todas las variables ni codificaciones ni informacion que tuvo el último modelo, así si lo corrieramos de vuelta, podríamos tener otro resultado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grilla: {'colsample_bytree': 0.7579620578446584, 'gamma': 0.7297041792253138, 'learning_rate': 0.021892166283765713, 'max_depth': 13, 'min_child_weight': 1.1724744683543746, 'n_estimators': 784, 'reg_lambda': 1.8656819555055764, 'subsample': 0.8630230443880356}\n"
     ]
    }
   ],
   "source": [
    "params = {'max_depth': list(range(1, 40)),\n",
    "          'learning_rate': uniform(scale = 0.2),\n",
    "          'gamma': uniform(scale = 2),\n",
    "          'reg_lambda': uniform(scale = 5),        # Parámetro de regularización.\n",
    "          'subsample': uniform(0.5, 0.5),          # Entre 0.5 y 1.\n",
    "          'min_child_weight': uniform(scale = 5),\n",
    "          'colsample_bytree': uniform(0.75, 0.25), # Entre 0.75 y 1.\n",
    "          'n_estimators': list(range(1, 1000))\n",
    "         }\n",
    "\n",
    "\n",
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "gc.collect()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include='number'),\n",
    "    train_data[\"conversion\"],\n",
    "    test_size=0.1,  # Proporción para el conjunto de prueba\n",
    "    random_state=42  # Semilla aleatoria para reproducibilidad\n",
    ")\n",
    "gc.collect()\n",
    "\n",
    "random_state = 2345\n",
    "start = time.time()\n",
    "best_score = 0\n",
    "best_estimator = None\n",
    "iterations = 100\n",
    "for g in ParameterSampler(params, n_iter = iterations, random_state = random_state):\n",
    "    clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = random_state, eval_metric = 'auc', **g)\n",
    "    clf_xgb.fit(X_train, y_train, eval_set = [(X_test, y_test)], verbose = False)\n",
    "\n",
    "    y_pred = clf_xgb.predict_proba(X_test)[:, 1] # Obtenemos la probabilidad de una de las clases (cualquiera).\n",
    "    auc_roc = sklearn.metrics.roc_auc_score(y_test, y_pred)\n",
    "    # Guardamos si es mejor.\n",
    "    if auc_roc > best_score:\n",
    "        print(f'Mejor valor de ROC-AUC encontrado: {auc_roc}')\n",
    "        best_score = auc_roc\n",
    "        best_grid = g\n",
    "        best_estimator = clf_xgb\n",
    "\n",
    "end = time.time()\n",
    "\n",
    "print('Grilla:', best_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjunto de validación\n",
    "\n",
    "Dada la gran cantidad de observaciones que tiene el dataset y para reducir los costos computacionales, optamos por usar el método de Holdout Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split into training and evaluation samples\n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data = comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "gc.collect()\n",
    "\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba (90% train, 10% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include=['number','float']),\n",
    "    train_data[\"conversion\"],\n",
    "    test_size=0.1,  # Proporción para el conjunto de prueba\n",
    "    random_state=42  # Semilla aleatoria para reproducibilidad\n",
    ")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenamiento del modelo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_depth': 13,\n",
    "          'learning_rate': 0.021892166283765713,\n",
    "          'gamma': 0.7297041792253138,\n",
    "          'reg_lambda': 1.8656819555055764,        \n",
    "          'subsample': 0.8630230443880356,          \n",
    "          'min_child_weight': 1.1724744683543746,\n",
    "          'colsample_bytree': 0.7579620578446584, \n",
    "          'n_estimators': 784\n",
    "         }\n",
    "\n",
    "\n",
    "clf_xgb = xgb.XGBClassifier(objective = 'binary:logistic', seed = 2345, eval_metric = 'auc', **params)\n",
    "clf_xgb.fit(X_train, y_train, eval_set = [(X_test, y_test)], verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance del modelo: 0.907064225258616\n"
     ]
    }
   ],
   "source": [
    "#Evaluamos la performance del modelo \n",
    "print(\"Performance del modelo:\",clf_xgb.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrenamos el modelo con todos los datos para hacer el subbmit \n",
    "train_data = comp_data[comp_data[\"ROW_ID\"].isna()]\n",
    "eval_data= comp_data[comp_data[\"ROW_ID\"].notna()]\n",
    "gc.collect()\n",
    "\n",
    "X_train = train_data.drop(columns=[\"conversion\", \"ROW_ID\"]).select_dtypes(include=['number','float'])\n",
    "y_train = train_data[\"conversion\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;background-color: white;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=&#x27;auc&#x27;, feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.7579620578446584, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric='auc', feature_types=None,\n",
       "              gamma=0.7297041792253138, gpu_id=None, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.021892166283765713, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=13, max_leaves=None,\n",
       "              min_child_weight=1.1724744683543746, missing=nan,\n",
       "              monotone_constraints=None, n_estimators=784, n_jobs=None,\n",
       "              num_parallel_tree=None, predictor=None, random_state=None, ...)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_depth': 13,\n",
    "          'learning_rate': 0.021892166283765713,\n",
    "          'gamma': 0.7297041792253138,\n",
    "          'reg_lambda': 1.8656819555055764,        \n",
    "          'subsample': 0.8630230443880356,          \n",
    "          'min_child_weight': 1.1724744683543746,\n",
    "          'colsample_bytree': 0.7579620578446584, \n",
    "          'n_estimators': 784\n",
    "         }\n",
    "\n",
    "\n",
    "clf_xgb2 = xgb.XGBClassifier(objective = 'binary:logistic', seed = 2345, eval_metric = 'auc', **params)\n",
    "clf_xgb2.fit(X_train, y_train, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_data = eval_data.drop(columns=[\"conversion\"])\n",
    "eval_data = eval_data.select_dtypes(include=['number','float'])\n",
    "y_preds = clf_xgb2.predict_proba(eval_data.drop(columns=[\"ROW_ID\"]))[:, clf_xgb2.classes_ == 1].squeeze()\n",
    "# Make the submission file\n",
    "submission_df = pd.DataFrame({\"ROW_ID\": eval_data[\"ROW_ID\"], \"conversion\": y_preds})\n",
    "submission_df[\"ROW_ID\"] = submission_df[\"ROW_ID\"].astype(int)\n",
    "submission_df.to_csv(\"xgboost_model_20.csv\", sep=\",\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
